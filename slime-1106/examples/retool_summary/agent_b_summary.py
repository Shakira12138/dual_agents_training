"""
Module: agent_b_summary
-----------------------
Agent B: Summary actor that compresses long context.
Processes training data from database (generated by Agent A) and generates summaries.

Functions:
    generate(...) -> Sample
    reward_func(...) -> float
"""

import asyncio
from typing import Dict, List

from slime.rollout.sglang_rollout import GenerateState
from slime.utils.http_utils import post
from slime.utils.types import Sample

from config import global_config
from summary_reward import compute_summary_score
from database_utils import get_retool_training_data


SUMMARY_SYSTEM_PROMPT = """ You are an expert at summarizing long conversations and technical content.
**Objective**
   - Generate a concise, faithful summary that preserves essential technical details and the logical flow.
   - Optimize for Completeness, Conciseness, Coherence, and Faithfulness.
   - Focus on preserving the original **train of thought**, not the raw logs.

**For `<tool_call> ... </tool_call>` and `<interpreter> ... </interpreter>` block**
   - Check whether there are a `<tool_call> ... </tool_call>` block immediately followed by an `<interpreter> ... </interpreter>` block.
   - For the last pair of `<tool_call> ... </tool_call>` and `<interpreter> ... </interpreter>` block, do the following:
     - **Copy these two blocks exactly as they appear in the input**, without any modification, truncation, or reformatting.
     - In your summary, keep them at the position that corresponds to their original place in the reasoning timeline (i.e., do not move them to the beginning or the end arbitrarily).

**All earlier tool calls and interpreter outputs should be compressed or dropped**
   - For every `<tool_call> ... </tool_call>` and `<interpreter> ... </interpreter>` block **except** the final pair described in (1) (when it exists):
     - Do **not** copy their raw contents.
     - Instead, summarize their role at a high level (e.g., “The assistant used Python to compute 8! = 40320”, or “The tool was used to factor 40320 into \(2^7 \cdot 3^2 \cdot 5 \cdot 7\)”).
     - Keep only **decisive results** (values, formulas, confirmations) that are necessary to understand or reproduce the reasoning.
   - Remove verbose logs, transient errors, and repeated re-definitions unless they are important to understand a mistake that was later corrected.


## Mandatory Rules

1) Faithfulness (HARD GATE)
   - Do not invent or alter facts. Do not speculate beyond what is stated.
   - If the conversation contains uncertainty or conflicting claims, state that briefly.
2) Completeness
   - Preserve the original chain of thought and any conclusions that explicitly appear in the conversation. If the conversation does not contain a clear conclusion, do not infer or construct new one.
   - Preserve essential technical details needed to reproduce or audit: formulas, APIs/parameters, IDs, key metrics, and decisive tool results.
3) Coherence
   - Maintain the reasoning chain: problem → approach/tools → key intermediate results → conclusion.
   - Organize with clear sections and short bullet points.
4) Conciseness
   - Maximize information density. Remove repetition, chit-chat, and irrelevant asides.
   - Keep only necessary code or tool outputs; trim logs to decisive lines.

Output Format (REQUIRED) — return ONLY markdown with these sections

Generate a concise summary that captures the essence of the conversation, without any new calculations, inferences, or conclusions that are not explicitly stated in the original conversation.
"""



def construct_summary_prompt(original_prompt: str, conversation_history: List[Dict]) -> str:
    """
    Construct prompt for summary generation.

    Args:
        original_prompt: Original user prompt
        conversation_history: List of conversation turns

    Returns:
        Formatted prompt for summary generation
    """
    conversation_text = ""
    for turn in conversation_history:
        role = turn.get("role", "assistant")
        content = turn.get("content", "")
        conversation_text += f"\n\n[{role.upper()}]\n{content}"

    full_prompt = f"""
Conversation History:
{conversation_text}

Please generate a comprehensive but concise summary that preserves the key information and reasoning flow."""

    return full_prompt


async def generate(args, sample: Sample, sampling_params) -> Sample:
    """
    Agent B generation function for summary creation (Database mode for offline training).

    This function is ONLY called by slime training loop for offline training.
    Router mode requests are handled directly by SGLang server, not through this function.

    Workflow:
    1. Fetch conversation data from database (generated by Agent A)
    2. Generate summary for the conversation
    3. Return Sample for training (slime will compute reward and train)

    Similar to patient_multiturn.generate in MrlX-TakesTwo.

    Args:
        args: Training arguments
        sample: Sample object (used for structure, data comes from database)
        sampling_params: Sampling parameters

    Returns:
        Sample with generated summary for training
    """
    # Get data from database (offline training mode)
    # Similar to patient_multiturn: get_patient_data()
    # This is called by slime training loop, not by router requests
    # Agent B completely relies on database, does NOT use sample.prompt from --prompt-data
    while True:
        training_data = get_retool_training_data()
        if training_data:
            # Extract data from database
            original_prompt = training_data.get("prompt", "")
            full_conversation = training_data.get("fullConversation", training_data.get("response", ""))
            label = training_data.get("label", "")
            task_id = training_data.get("id", "")

            # Build conversation history for summary
            # The full conversation (prompt + response) will be summarized
            conversation_history = [{"role": "assistant", "content": full_conversation}]

            # Approximate token count
            token_count = len(full_conversation.split())

            print(f"[Agent B] Processing training data from database: conversation length = {len(full_conversation)}")
            break
        else:
            # Database is empty, wait and retry (do NOT use sample.prompt)
            print("[Agent B] No unprocessed samples in database, waiting 5 seconds...")
            await asyncio.sleep(5)

    # Construct summary prompt
    summary_prompt = construct_summary_prompt(original_prompt, conversation_history)

    # Generate summary
    state = GenerateState(args)
    url = f"http://{args.sglang_router_ip}:{args.sglang_router_port}/generate"

    # Apply chat template with system prompt
    messages = [
        {"role": "system", "content": SUMMARY_SYSTEM_PROMPT},
        {"role": "user", "content": summary_prompt},
    ]

    formatted_prompt = state.tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False,
    )

    # Generate summary
    payload = {
        "text": formatted_prompt,
        "sampling_params": sampling_params,
    }

    output = await post(url, payload)

    # Handle abort
    if output["meta_info"]["finish_reason"]["type"] == "abort":
        sample.status = Sample.Status.ABORTED
        sample.reward = global_config.DEFAULT_SCORE
        print(f"[Agent B] Summary generation aborted")
        return sample

    summary_text = output["text"].strip()

    # Tokenize for sample
    prompt_tokens = state.tokenizer(formatted_prompt, add_special_tokens=False)["input_ids"]
    response_tokens = state.tokenizer(summary_text, add_special_tokens=False)["input_ids"]

    # Fill sample object
    sample.prompt = summary_prompt  # Store original prompt for reward calculation
    sample.tokens = prompt_tokens + response_tokens
    sample.response_length = len(response_tokens)
    sample.response = summary_text
    sample.loss_mask = [1] * len(response_tokens)

    # Store metadata for reward calculation
    sample.metadata["task_id"] = task_id
    sample.metadata["original_prompt"] = original_prompt
    sample.metadata["full_conversation"] = full_conversation if full_conversation else original_prompt
    sample.metadata["conversation_history"] = conversation_history
    sample.metadata["original_token_count"] = token_count
    sample.metadata["summary_token_count"] = len(response_tokens)
    sample.metadata["label"] = label  # Store label for reward calculation

    # Set status
    if output["meta_info"]["finish_reason"]["type"] == "length":
        sample.status = Sample.Status.TRUNCATED
    else:
        sample.status = Sample.Status.COMPLETED

    print(f"[Agent B] Summary generated: {len(response_tokens)} tokens")

    return sample


async def reward_func(args, sample, **kwargs):
    """
    Summary reward function using LLM-as-judge.

    Evaluates summary quality based on:
    - Completeness: Does it capture key information?
    - Conciseness: Is it appropriately compressed?
    - Coherence: Is it well-structured and logical?

    Args:
        args: Training arguments
        sample: Sample object with summary

    Returns:
        Reward score (float)
    """
    if not isinstance(sample, Sample):
        raise TypeError("Sample must be an instance of Sample class.")

    # Extract metadata
    original_prompt = sample.metadata.get("original_prompt", "")
    conversation_history = sample.metadata.get("conversation_history", [])
    original_token_count = sample.metadata.get("original_token_count", 0)
    summary_token_count = sample.metadata.get("summary_token_count", 0)

    # Compute summary quality score using LLM-as-judge
    score_details = await compute_summary_score(
        original_prompt=original_prompt,
        conversation_history=conversation_history,
        summary=sample.response,
        original_token_count=original_token_count,
        summary_token_count=summary_token_count,
    )

    total_score = score_details.get("total_score", 0.0)

    print(f"[Agent B] Summary reward: {total_score:.3f} (details: {score_details})")

    return total_score



